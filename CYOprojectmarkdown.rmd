---
title: "Diabetes Risk Prediction - CYO Project"
author: "Poonam Quraishy"
date: "2/22/2022"
output: 
  pdf_document:
    toc: true
    df_print: kable
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



# Overview 


This project is the Choose your own section of the Data Science Capstone offered by HarvardX. The aim of this project is to apply machine learning techniques on any chosen dataset and present the insights and analysis in a structured report.


# Introduction  

Diabetes mellitus is a chronic health condition that occurs when the pancreas do not produce enough insulin or the body cannot efficiently use the insulin it produces. Chronic diabeteic conditions include Type 1 diabetes, Type 2 diabetes, prediabetes, and gestational diabetes. The incidence and prevalence of diabetes mellitus is rapidly growing and has already affected 422 million people as stated by a report by the World Health Organization (WHO), in 2018. According to the World Health Organization, diabetes can be treated and its consequences avoided or delayed with diet, physical activity, medication, regular screening, and treatment for complications. Early detection of diabetes is ideally desired for a clinically meaningful outcome. Diabetes has a relatively long asymptomatic phase which poses challenges to early detection and diagnosis. 
    This project attempts to create multiple machine learning models to predict the risk of developing diabetes. The modeling techniques performed on this dataset include Logistic Regression, Classification Trees, eXtreme Gradient Boosing (XGBoost), K - Nearest Neighbors, and Support Vector Machines (SVM). 


```{r, include=FALSE, echo=FALSE}

# Install and load all the required libraries #

if(!require(tidyverse)) install.packages("tidyverse")
if(!require(kableExtra)) install.packages("kableExtra")
if(!require(tidyr)) install.packages("tidyr")
if(!require(knitr)) install.packages("knitr")
if(!require(rpart)) install.packages("rpart")
if(!require(rpart.plot)) install.packages("rpart.plot")
if(!require(caret)) install.packages("caret")
if(!require(grid)) install.packages("grid")
if(!require(gridExtra)) install.packages("gridExtra")
if(!require(gbm)) install.packages("gbm")
if(!require(xgboost)) installed.packages("xgboost")
if(!require(e1071)) install.packages("e1071")
if(!require(corrplot)) install.packages("corrplot")
if(!require(MLeval)) install.packages("MLeval")
if(!require(ranger)) install.packages("ranger")
if(!require(quantmod)) install.packages("quantmod")
if(!require(kernlab)) install.packages("kernlab")
if(!require(markdown)) install.packages("markdown")
if(!require(rmarkdown)) install.packages("rmarkdown")
if(!require(stringr)) install.packages("stringr")
if(!require(pROC)) install.packages("pROC")
if(!require(ggplot2)) install.packages("ggplot2")
if(!require(neuralnet)) install.packages("neuralnet")
if(!require(gbm)) install.packages("gbm")
if(!require(dplyr)) install.packages("dplyr")
if(!require(MLeval)) install.packages("MLeval")
if(!require(caret)) install.packages("caret")
if(!require(xgboost)) install.packages("xgboost")
if(!require(roc))
if(!require(class)) install.packages("class")
if(!require(ROCR)) install.packages("ROCR")
if(!require(randomForest)) install.packages("randomForest")
if(!require(reshape2)) install.packages("reshape2")
if(!require(mlbench)) install.packages("mLbench")


# Loading required libraries
library(tidyverse)
library(kableExtra)
library(tidyr)
library(knitr)
library(gbm)
library(caret)
library(MLeval)
library(rpart)
library(pROC)
library(rpart.plot)
library(grid)
library(gridExtra)
library(e1071)
library(xgboost)
library(corrplot)
library(mlbench)
library(MLeval)
library(quantmod)
library(kernlab)
library(ranger)
library(markdown)
library(neuralnet)
library(rmarkdown)
library(stringr)
library(ggplot2)
library(gbm)
library(dplyr)
library(caret)
library(xgboost)
library(class)
library(ROCR)
library(randomForest)
library(reshape2)
```

# Dataset 

The dataset used for this project is the Pima Indians Diabetes dataset available in the mLbench package.
This dataset contains 786 observations and 9 variables. The observations are PIMA Indian females near Pheonix Arizona. The 9 variables are as follows -
1.Pregnant
2.Glucose
3.Pressure
4.Triceps
5.Insulin
6.Mass
7.Pedigree
8.Age
9.Diabetes

'Diabetes' will be the response/target variable. The diabetes variable contains 
500 negative and 268 positive outcomes which indicate if the person was diagnosed with
diabetes or not. The data set will be split into an 80-20 train and test data set.
    





# Exploratory Analysis 


The Pima Indians Diabetes dataset is available in the package mLbench.

```{r, include=TRUE, echo=FALSE}


data(PimaIndiansDiabetes)

pimadf <- PimaIndiansDiabetes

str(pimadf)
```
First few rows

```{r, include=TRUE, echo=FALSE}

head(pimadf)
```


Names of the columns 
```{r, include=TRUE, echo=FALSE}

colnames(pimadf)
```

Summary of the dataset
```{r, include=TRUE, echo=FALSE}

summary(pimadf)
```

Dimensions 
```{r, include=TRUE, echo=FALSE}

dim(pimadf)
```

Check for NA's or missing values
```{r, include=TRUE, echo=FALSE}

sapply(pimadf, function(x) sum(is.na(x)))
```

Exploring the response variable Diabetes
```{r, include=TRUE, echo=FALSE}

pimadf$diabetes <- factor(pimadf$diabetes)

```

Diabetes - The sample has a high occurrence of positive diabetes diagnosis.

```{r,fig.align='center', include=TRUE, echo=FALSE}

ggplot(pimadf,aes(diabetes,fill = diabetes)) +
  geom_bar() + 
  ggtitle("Distribution of the diabetes variable")
```

Plot of Correlations between all the predictor variables
```{r,fig.align='center', include=TRUE, echo=FALSE}

corrmatx <- cor(pimadf[, -9])

corrplot.mixed(corrmatx,tl.pos = "lt")
```

Numerical Representation
```{r, include=TRUE, echo=FALSE}

corrplot::corrplot(corrmatx, type = "lower", method = "number")
```

Univariate Analysis 
```{r,fig.align='center', include=TRUE, echo=FALSE}

bivar_plot <- function(bivar_name, bivar, data, output_var) {
  
  p_1 <- ggplot(data = data, aes(x = bivar, fill = output_var)) +
    geom_bar(stat='count', position='identity') +
    theme_bw() +
    labs( title = paste(bivar_name,"- Diabetes", sep =" "), x = bivar_name) +
    theme(plot.title = element_text(hjust = 0.5))
  
  plot(p_1)
}

for (x in 1:(ncol(pimadf)-1)) {
  bivar_plot(bivar_name = names(pimadf)[x], bivar = pimadf[,x], data = pimadf, output_var = pimadf[,'diabetes'])
}
```

It is evident that high glucose levels lead to a higher chance of positive diabetes diagnosis.
Mass/BMI increases also increase the chance of a positive diabetes diagnosis.
Age over the age of 25 also an indicator that increases the chance of a diabetes diagnosis.
There is no notable significant distinction among other variables to warrant further exploration.

Exploring the Age variable
```{r,fig.align='center', include=TRUE, echo=FALSE}

p2 <- ggplot(pimadf, aes(age, fill = diabetes)) +
  geom_histogram(binwidth = 5) +
  theme(legend.position = "bottom") +
  ggtitle("Age of women Vs Diabetes")

p1 <- ggplot(pimadf, aes(x = diabetes, y = age,fill = diabetes)) +
  geom_boxplot() +
  theme(legend.position = "bottom") +
  ggtitle("Age of women Vs Diabetes")

gridExtra::grid.arrange(p1, p2, ncol = 2)
```

Exploring the mass variable
```{r,fig.align='center',  include=TRUE, echo=FALSE}

m1 <- ggplot(pimadf, aes(mass, fill = diabetes)) +
  geom_histogram() +
  theme(legend.position = "bottom") +
  ggtitle("Mass of women Vs Diabetes")

m2 <- ggplot(pimadf, aes(x = diabetes, y = mass,fill = diabetes)) +
  geom_boxplot(binwidth = 30)  +
  theme(legend.position = "bottom") +
  ggtitle("Mass of women Vs Diabetes")

gridExtra::grid.arrange(m2, m1, ncol = 2)
```

Exploring the Glucose variable
```{r,fig.align='center', include=TRUE, echo=FALSE}

d1 <- ggplot(pimadf, aes(x = glucose, color = diabetes, fill = diabetes)) +
  geom_density(alpha = 0.8) +
  theme(legend.position = "bottom") +
  labs(x = "Glucose", y = "Density", title = "Density plot of glucose")

d2 <- ggplot(pimadf, aes(x = diabetes, y = glucose,fill = diabetes)) +
  geom_boxplot() +
  theme(legend.position = "bottom") +
  ggtitle("Glucose levels Vs diabetes")

gridExtra::grid.arrange(d2, d1, ncol = 2)
```


#  Modeling Approach 


Split the Data into Training set consisting of 80% of the data and Testing set 
consisting of 20% of the data.

```{r, include=TRUE, echo=FALSE}

partition <- caret::createDataPartition(y = pimadf$diabetes, times = 1, p = 0.8, list = FALSE)
```

Training set and Test set
```{r, include=TRUE, echo=FALSE}
train_set <- pimadf[partition,]

test_set <- pimadf[-partition,]

str(train_set)

summary(train_set)
summary(test_set)
```

################################################################################
## Model 1 - Logistic Regression.  
################################################################################
Logistic Regression is an introductory classification algorithm used to find the probability of event success and event failure. Logistic regression is used when the dependent variable is binary in nature.

```{r, include=TRUE, echo=TRUE}

glm_model <- caret::train(diabetes ~., data = train_set,
                          method = "glm",
                          metric = "ROC",
                          tuneLength = 10,
                          trControl = trainControl(method = "cv", number = 10,
                                                   classProbs = T, summaryFunction = twoClassSummary),
                          preProcess = c("center","scale","pca"))

```

Final ROC for the Logistic Regression 
```{r, include=TRUE, echo=FALSE}
glm_model$results[1,2]
```


################################################################################
## Model 2 - Classification Tree.  
################################################################################

A classification tree is used for modeling data when the response variable is categorical. The tree splits the data into two or more homogeneous sets based on the most significant differentiator in the predictor variables-value set. We will build a classification tree using the binary response variable 'diabetes'.


```{r,fig.align='center', include=TRUE, echo=TRUE}
tree_model <- rpart(diabetes~., data=train_set, method="class")
tree_model
rpart.plot(tree_model)
```

Best Model Complexity Parameter - CP
```{r,fig.align='center', include=TRUE, echo=FALSE}
plotcp(tree_model)
```

The complexity parameter value of 0.015 was chosen since the relative error does not decrease significantly after this value. 

Model after Pruning the tree
```{r,fig.align='center', include=TRUE, echo=TRUE}
tree_model <- rpart(diabetes~., data=train_set, method="class",cp=0.015)
rpart.plot(tree_model)
```

################################################################################
## Model 3 - XGBoost - eXtreme Gradient Boosting  
################################################################################

eXtreme Gradient Boosting Machine (XGBoost) is a popular machine learning algorithm that can be used for both Regression and Classification. Gradient boosting is an approach where new models are created that predict the residuals or errors of prior models and then added together to make the final prediction. It is called gradient boosting because it uses a gradient descent algorithm to minimize the loss when adding new models.

```{r, include=TRUE, echo=TRUE}

xgb_grid  <-  expand.grid(
                         nrounds = 50,
                         eta = c(0.03),
                         max_depth = 1,
                         gamma = 0,
                         colsample_bytree = 0.6,
                         min_child_weight = 1,
                         subsample = 0.5
)

xgb_model <- caret::train(diabetes ~., data = train_set,
                          method = "xgbTree",
                          metric = "ROC",
                          tuneGrid=xgb_grid,
                          trControl = trainControl(method = "cv", number = 10,
                                                   classProbs = T, summaryFunction = twoClassSummary),
                          preProcess = c("center","scale","pca"))
xgb_model

xgb_model$results["ROC"]
```

################################################################################
## Model 4 - K Nearest Neighbors (KNN)  
################################################################################

The K-Nearest Neighbors (KNN) algorithm is a supervised machine learning algorithm that can be used to solve both classification and regression problems.The principle behind this technique is that known data are arranged in a space defined by the selected features. When a new data is supplied to the algorithm, the algorithm will compare the classes of the k closest data to determine the class of the new data.

```{r,fig.align='center', include=TRUE, echo=TRUE}
knn_model <- caret::train(diabetes ~., data = train_set,
                          method = "knn",
                          metric = "ROC",
                          tuneGrid = expand.grid(.k = c(3:10)),
                          trControl = trainControl(method = "cv", number = 10,
                                                   classProbs = T, summaryFunction = twoClassSummary),
                          preProcess = c("center","scale","pca"))

knn_model
plot(knn_model)

knn_model$results[7,2]
```

################################################################################
## Model 5 - Support vector machine (SVM) 
################################################################################


The objective of the support vector machine algorithm is to find a hyperplane in an N-dimensional space(N â€” the number of features) that distinctly classifies the data points. SVM focuses on the dual aspects of maximizing the minimum margin between the hyperplane and support vectors; and minimizing the misclassification rate.


Selecting the best parameters using the tune() function to do a grid search over the supplied parameter ranges (C - cost, gamma), using the train set. 
The range to gamma parameter is between 0.000001 and 0.1. 
For cost parameter the range is from 0.1 until 10.

```{r, include=TRUE, echo=TRUE}

model_svmtune <- tune.svm(diabetes ~., data = train_set, gamma = 10^(-6:-1), cost = 10^(-1:1))
summary(model_svmtune) # to show the results

# As we can see the result show that the best parameters are Cost=10 and gamma=0.01.

svm_model  <- svm(diabetes ~., data = train_set, kernel = "radial", gamma = 0.01, cost = 10, probability = TRUE) 
summary(svm_model)
```



################################################################################
### Prediction on the Test data set. 
################################################################################

### Model 1 - Logistic Regression


Prediction on Test data set

```{r, include=TRUE, echo=FALSE}
pred_glm <- predict(glm_model, test_set)

# Confusion Matrix 
cmx_glm <- confusionMatrix(pred_glm, test_set$diabetes, positive="pos")


pred_prob_glm <- predict(glm_model, test_set, type="prob")
# ROC value
roc_glm <- roc(test_set$diabetes, pred_prob_glm$pos)

# Confusion matrix 
cmx_glm

# ROC

roc_glm
```

### Model 2 - Classification tree


Prediction on the test data set.

```{r, include=TRUE, echo=FALSE}
pred_rpart <- predict(tree_model, test_set, type = "class")

# prediction probabilities
tree_prob_pred <- predict(tree_model, test_set, type = "prob")

# confusion matrix
cmx_tree <- confusionMatrix(test_set$diabetes, pred_rpart)

cmx_tree
```

### Model 3 - XGBoost

Prediction on Test data set

```{r, include=TRUE, echo=FALSE}
pred_xgb <- predict(xgb_model, test_set)

# Confusion Matrix 
cmx_xgb <- confusionMatrix(pred_xgb, test_set$diabetes, positive="pos")


pred_prob_xgb <- predict(xgb_model, test_set, type="prob")
# ROC value
roc_xgb <- roc(test_set$diabetes, pred_prob_xgb$pos)

# Confusion matrix 
cmx_xgb
```


### Model 4 - KNN

Prediction on Test data set

```{r, include=TRUE, echo=FALSE}
pred_knn <- predict(knn_model, test_set)

# Confusion Matrix 
cmx_knn <- confusionMatrix(pred_knn, test_set$diabetes, positive="pos")

pred_prob_knn <- predict(knn_model, test_set, type="prob")
# ROC value
roc_knn <- roc(test_set$diabetes, pred_prob_knn$pos)

# Confusion matrix 
cmx_knn
```

### Model 5 - SVM

Prediction on the test data

```{r, include=TRUE, echo=FALSE}
par(mfrow = c(1,2))
pred_svm <- predict(svm_model, test_set)


svm_prob_pred <- predict(svm_model,test_set,probability = TRUE)

# Confusion matrix
cmx_svm <- confusionMatrix(test_set$diabetes, svm_prob_pred)

cmx_svm
```

# Comparing the Test results of all the models.


```{r, include=TRUE, echo=FALSE}
test_glm <- c(cmx_glm$byClass['Sensitivity'], cmx_glm$byClass['Specificity'], cmx_glm$byClass['Precision'], 
                cmx_glm$byClass['Recall'], cmx_glm$byClass['F1'])

test_tree <- c(cmx_tree$byClass['Sensitivity'], cmx_tree$byClass['Specificity'], cmx_tree$byClass['Precision'], 
                  cmx_tree$byClass['Recall'], cmx_tree$byClass['F1'])


test_xgb <- c(cmx_xgb$byClass['Sensitivity'], cmx_xgb$byClass['Specificity'], cmx_xgb$byClass['Precision'], 
                cmx_xgb$byClass['Recall'], cmx_xgb$byClass['F1'])

test_knn <- c(cmx_knn$byClass['Sensitivity'], cmx_knn$byClass['Specificity'], cmx_knn$byClass['Precision'], 
                cmx_knn$byClass['Recall'], cmx_knn$byClass['F1'])

test_svm <- c(cmx_svm$byClass['Sensitivity'], cmx_svm$byClass['Specificity'], cmx_svm$byClass['Precision'], 
                cmx_svm$byClass['Recall'], cmx_svm$byClass['F1'])
```

# Results 
```{r, include=TRUE, echo=FALSE}

test_results <- data.frame(rbind(test_glm, test_tree, test_knn, test_xgb, test_svm))
names(test_results) <- c("Sensitivity", "Specificity", "Precision", "Recall", "F1")
test_results
```
# Comparing the accuracy of all the models

```{r, include=TRUE, echo=FALSE}

glm_accuracy <- confusionMatrix(pred_glm, test_set$diabetes)$overall['Accuracy']

tree_accuracy <- confusionMatrix(pred_rpart, test_set$diabetes)$overall['Accuracy']

xgb_accuracy <- confusionMatrix(pred_xgb, test_set$diabetes)$overall['Accuracy']

knn_accuracy <- confusionMatrix(pred_knn, test_set$diabetes)$overall['Accuracy']

svm_accuracy <- confusionMatrix(pred_svm, test_set$diabetes)$overall['Accuracy']

### Comparing the accuracy of all the models

accuracy <- data.frame(Model=c("Logistic Regression","Classification Tree","XGboost","K nearest neighbor (KNN)", "Support Vector Machine (SVM)"), 
                       Accuracy=c(glm_accuracy, tree_accuracy, xgb_accuracy, knn_accuracy, svm_accuracy ))

accuracy


ggplot(accuracy,aes(x=Model,y=Accuracy)) + geom_bar(stat='identity') + theme_bw() + ggtitle('Comparison of Model Accuracy')+
  geom_col(fill = "coral1") +  geom_jitter(width=0.15) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```
After comparing the results it is evident that all the models have pros and cons and no single model has a perfect combination that makes it superior to the other models.

# Conclusion

The Pima Indian dataset was explored and analyzed in detail. Multiple machine learning models were built and tested in order to identify the best performing model to predict the occurrence of diabetes in Pima Indian women. According to the results the Logistic Regression, and SVM performed similarly in terms of accuracy. Accuracy can be defined as the percentage of correct predictions for the test data. The Support vector machine model and Classification tree performed best in terms of Recall which can be defined as the fraction of examples which were predicted to belong to a class with respect to all of the examples that truly belong in the class. The SVM model also performed best on Precision which tells us about the percentage of positive instances out of the total predicted positive instances. The SVM and Classification tree  model performed best in terms of Sensitivity which is the ability of the test to correctly identify the true positive rate. The Extreme gradient boosting model performed the best in terms of Specificity which is the ability of the test to correctly identify the true negative rate. The SVM model also had the highest F1 score which can be defined as the harmonic mean of precision and recall. The findings suggest that Support vector machine modeling is a promising classification approach for detecting persons with diabetes in the population. This approach should be further explored in other complex diseases using common variables.

# Enviroment

```{r, echo=FALSE, include=TRUE}
print("Operating System:")
version
```

```


